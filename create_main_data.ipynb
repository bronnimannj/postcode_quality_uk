{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the main data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages import\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n",
      "1.23.3\n",
      "3.6.0\n"
     ]
    }
   ],
   "source": [
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "import matplotlib\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import\n",
    "First, we import the filtered tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/filtered/postcode_filtered.zip')\n",
    "pets = pd.read_csv('data/filtered/pets_filtered.zip')\n",
    "flood = pd.read_csv('data/filtered/flood_risk_filtered.zip')\n",
    "imd = pd.read_csv('data/filtered/imd_filtered.zip')\n",
    "elevation = pd.read_csv('data/filtered/elevation_filtered.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_starting = data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join data\n",
    "### Create district and sector data\n",
    "From the postcode data, we create district and sector datasets to make sure we have the full list of districts and sectors to join other tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the district and sector columns\n",
    "data[['district', 'dis1']] = data['pcds'].str.split(pat = ' ',n = 1, expand=True)\n",
    "data.drop(['dis1'], axis = 1, inplace = True)\n",
    "\n",
    "data['sector'] = data['district'].str[0:2]\n",
    "data['sector'] = data['sector'].str.replace(\n",
    "    pat = r\"[0-9]+\", \n",
    "    repl = \"\",\n",
    "    regex = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data with one row per district\n",
    "district_data = data[['district','sector']].\\\n",
    "    drop_duplicates().\\\n",
    "    reset_index(drop = True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data with one row per sector\n",
    "lsoa_data = data[['lsoa11']].\\\n",
    "    drop_duplicates().\\\n",
    "    reset_index(drop = True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create full data\n",
    "Now, let's create full tables, as most of them do not cover all postcodes/districts/sectors.\n",
    "#### Pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the district data to pets\n",
    "pets_pcds = district_data.merge(\n",
    "    pets,\n",
    "    on = 'district',\n",
    "    how = 'left'\n",
    ")\n",
    "\n",
    "# let's take the districts that do contain values\n",
    "pets_existing = pets_pcds[~pets_pcds['estimated_cat_population'].isna()].copy()\n",
    "pets_existing['pets_value_from'] = 'raw_district'\n",
    "\n",
    "# rows that have missing values\n",
    "pets_missing = pets_pcds[pets_pcds['estimated_cat_population'].isna()].copy()\n",
    "\n",
    "# # group the existing values by sector and average the values\n",
    "pets_sector_avg = pets_existing.groupby(['sector'])\\\n",
    "    .agg({\n",
    "        'estimated_cat_population': 'mean',\n",
    "        'dog_per_household_lower95': 'mean'\n",
    "        }\n",
    "    )\n",
    "pets_missing = pets_missing[['district','sector']].merge(\n",
    "    pets_sector_avg,\n",
    "    how = 'inner',\n",
    "    on = 'sector'\n",
    ")\n",
    "pets_missing['pets_value_from'] = 'avg_sector'\n",
    "\n",
    "\n",
    "# put data back together\n",
    "pets_pcds = pd.concat(\n",
    "    [pets_existing,pets_missing]\n",
    ")\n",
    "\n",
    "# clean memory\n",
    "del pets_existing, pets_missing, pets_sector_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imd.rename(\n",
    "    columns = {'lsoa':'lsoa11'},\n",
    "    inplace = True\n",
    ")\n",
    "\n",
    "# join the lsoa data to IMD data\n",
    "lsoa_data = lsoa_data.merge(\n",
    "    imd,\n",
    "    on = 'lsoa11',\n",
    "    how = 'inner'\n",
    ")\n",
    "\n",
    "del imd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on all postcodes\n",
    "flood_pcd = data[['pcds', 'district','sector']].merge(\n",
    "    flood,\n",
    "    how = 'left',\n",
    "    left_on = 'pcds',\n",
    "    right_on = 'postcode'\n",
    ")\n",
    "\n",
    "# CREATE SUB TABLES BY DISTRICT, OR SECTOR\n",
    "\n",
    "# take the subdata where values are not missing and group by district\n",
    "flood_dst = flood_pcd[ ~flood_pcd['flood_risk_int'].isna()]\\\n",
    "    .groupby(['district'])\\\n",
    "    .agg({\n",
    "        'flood_risk_int': 'mean',\n",
    "        'risk_for_insurance_int': 'mean'\n",
    "        }\n",
    "    )\n",
    "# round to nearest int\n",
    "flood_dst['flood_risk_int'] = np.round(flood_dst['flood_risk_int'],0)\n",
    "flood_dst['risk_for_insurance_int'] = np.round(flood_dst['risk_for_insurance_int'],0)\n",
    "\n",
    "# take the subdata where values are not missing and group by sector\n",
    "flood_sct = flood_pcd[ ~flood_pcd['flood_risk_int'].isna()]\\\n",
    "    .groupby(['sector'])\\\n",
    "    .agg({\n",
    "        'flood_risk_int': 'mean',\n",
    "        'risk_for_insurance_int': 'mean'\n",
    "        }\n",
    "    )\n",
    "# round to nearest int\n",
    "flood_sct['flood_risk_int'] = np.round(flood_sct['flood_risk_int'],0)\n",
    "flood_sct['risk_for_insurance_int'] = np.round(flood_sct['risk_for_insurance_int'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 0 rows with missing sector values of flood\n"
     ]
    }
   ],
   "source": [
    "### CREATE THE SUBSETS OF OUTPUT TABLE\n",
    "\n",
    "# by postcode, when value exist by postcode\n",
    "flood_pcd_e = flood_pcd[ ~flood_pcd['flood_risk_int'].isna()].copy()\n",
    "flood_pcd_e['flood_value_from'] = 'raw_postcode'\n",
    "\n",
    "# when value missing by postcode\n",
    "flood_pcd_m = flood_pcd[ flood_pcd['flood_risk_int'].isna()].copy()\n",
    "\n",
    "# join flood_pcd_m to district values\n",
    "flood_pcd_m_dst = flood_pcd_m[[\"pcds\",\"district\", \"sector\"]].merge(\n",
    "    flood_dst,\n",
    "    on = 'district',\n",
    "    how = 'left'\n",
    ")\n",
    "\n",
    "# by district, when value exist averaged by district\n",
    "flood_pcd_m_dst_e = flood_pcd_m_dst[ ~flood_pcd_m_dst['flood_risk_int'].isna()].copy()\n",
    "flood_pcd_m_dst_e['flood_value_from'] = 'average_district'\n",
    "\n",
    "# when value is missing by district\n",
    "flood_pcd_m_dst_m = flood_pcd_m_dst[ flood_pcd_m_dst['flood_risk_int'].isna()].copy()\n",
    "\n",
    "# join flood_sct to sector values\n",
    "flood_pcd_m_dst_m_sct = flood_pcd_m_dst_m[[\"pcds\",\"sector\"]].merge(\n",
    "    flood_sct,\n",
    "    on = \"sector\",\n",
    "    how = \"left\"\n",
    ")\n",
    "\n",
    "# by sector, when value exist averaged by sector\n",
    "flood_pcd_m_dst_m_sct_e = flood_pcd_m_dst_m_sct[ ~flood_pcd_m_dst_m_sct['flood_risk_int'].isna()].copy()\n",
    "flood_pcd_m_dst_m_sct_e['flood_value_from'] = 'average_sector'\n",
    "\n",
    "# when value is missing by sector\n",
    "flood_pcd_m_dst_m_sct_m = flood_pcd_m_dst_m_sct[ flood_pcd_m_dst_m_sct['flood_risk_int'].isna()].copy()\n",
    "\n",
    "print(f\"We have {flood_pcd_m_dst_m_sct_m.shape[0]} rows with missing sector values of flood\")\n",
    "\n",
    "# group together\n",
    "flood_final = pd.concat(\n",
    "    [\n",
    "        flood_pcd_e,\n",
    "        flood_pcd_m_dst_e,\n",
    "        flood_pcd_m_dst_m_sct_e\n",
    "    ]\n",
    ")\n",
    "\n",
    "# delete un-necessary \n",
    "# del (flood_dst, flood_sct, flood_pcd_e, flood_pcd_m, flood_pcd_m_dst,\n",
    "#      flood_pcd_m_dst_e, flood_pcd_m_dst_m, flood_pcd_m_dst_m_sct,\n",
    "#      flood_pcd_m_dst_m_sct_e, flood_pcd_m_dst_m_sct_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on all postcodes\n",
    "elev_pcd = data[['pcds', 'district','sector']].merge(\n",
    "    elevation,\n",
    "    how = 'left',\n",
    "    left_on = 'pcds',\n",
    "    right_on = 'postcode'\n",
    ")\n",
    "\n",
    "# CREATE SUB TABLES BY DISTRICT, OR SECTOR\n",
    "\n",
    "# take the subdata where values are not missing and group by district\n",
    "elev_dst = elev_pcd[ ~elev_pcd['elevation'].isna()]\\\n",
    "    .groupby(['district'])\\\n",
    "    .agg({\n",
    "        'elevation': 'mean'\n",
    "        }\n",
    "    )\n",
    "# round to nearest int\n",
    "elev_dst['elevation'] = np.round(elev_dst['elevation'],0)\n",
    "\n",
    "# take the subdata where values are not missing and group by sector\n",
    "elev_sct = elev_pcd[ ~elev_pcd['elevation'].isna()]\\\n",
    "    .groupby(['sector'])\\\n",
    "    .agg({\n",
    "        'elevation': 'mean'\n",
    "        }\n",
    "    )\n",
    "# round to nearest int\n",
    "elev_sct['elevation'] = np.round(elev_sct['elevation'],0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 0 rows with missing sector values of elevation\n"
     ]
    }
   ],
   "source": [
    "### CREATE THE SUBSETS OF OUTPUT TABLE\n",
    "\n",
    "# by postcode, when value exist by postcode\n",
    "elev_pcd_e = elev_pcd[ ~elev_pcd['elevation'].isna()].copy()\n",
    "elev_pcd_e['elevation_value_from'] = 'raw_postcode'\n",
    "\n",
    "# when value missing by postcode\n",
    "elev_pcd_m = elev_pcd[ elev_pcd['elevation'].isna()].copy()\n",
    "\n",
    "# join elev_pcd_m to district values\n",
    "elev_pcd_m_dst = elev_pcd_m[[\"pcds\",\"district\", \"sector\"]].merge(\n",
    "    elev_dst,\n",
    "    on = 'district',\n",
    "    how = 'left'\n",
    ")\n",
    "\n",
    "# by district, when value exist averaged by district\n",
    "elev_pcd_m_dst_e = elev_pcd_m_dst[ ~elev_pcd_m_dst['elevation'].isna()].copy()\n",
    "elev_pcd_m_dst_e['elevation_value_from'] = 'average_district'\n",
    "\n",
    "# when value is missing by district\n",
    "elev_pcd_m_dst_m = elev_pcd_m_dst[ elev_pcd_m_dst['elevation'].isna()].copy()\n",
    "\n",
    "# join elev_sct to sector values\n",
    "elev_pcd_m_dst_m_sct = elev_pcd_m_dst_m[[\"pcds\",\"sector\"]].merge(\n",
    "    elev_sct,\n",
    "    on = \"sector\",\n",
    "    how = \"left\"\n",
    ")\n",
    "\n",
    "# by sector, when value exist averaged by sector\n",
    "elev_pcd_m_dst_m_sct_e = elev_pcd_m_dst_m_sct[ ~elev_pcd_m_dst_m_sct['elevation'].isna()].copy()\n",
    "elev_pcd_m_dst_m_sct_e['elevation_value_from'] = 'average_sector'\n",
    "\n",
    "# when value is missing by sector\n",
    "elev_pcd_m_dst_m_sct_m = elev_pcd_m_dst_m_sct[ elev_pcd_m_dst_m_sct['elevation'].isna()].copy()\n",
    "\n",
    "print(f\"We have {elev_pcd_m_dst_m_sct_m.shape[0]} rows with missing sector values of elevation\")\n",
    "\n",
    "# group together\n",
    "elev_final = pd.concat(\n",
    "    [\n",
    "        elev_pcd_e,\n",
    "        elev_pcd_m_dst_e,\n",
    "        elev_pcd_m_dst_m_sct_e\n",
    "    ]\n",
    ")\n",
    "\n",
    "# delete un-necessary \n",
    "del (elev_dst, elev_sct, elev_pcd_e, elev_pcd_m, elev_pcd_m_dst,\n",
    "     elev_pcd_m_dst_e, elev_pcd_m_dst_m, elev_pcd_m_dst_m_sct,\n",
    "     elev_pcd_m_dst_m_sct_e, elev_pcd_m_dst_m_sct_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data\\\n",
    "    .merge(\n",
    "        pets_pcds[['district','estimated_cat_population','dog_per_household_lower95','pets_value_from']],\n",
    "        on = 'district',\n",
    "        how = 'left'\n",
    "    )\\\n",
    "    .merge(\n",
    "        lsoa_data,\n",
    "        on = 'lsoa11',\n",
    "        how = 'left'\n",
    "    )\\\n",
    "    .merge(\n",
    "        flood_final[['pcds','flood_risk_int','risk_for_insurance_int','flood_value_from']],\n",
    "        on = 'pcds',\n",
    "        how = 'left'\n",
    "    )\\\n",
    "    .merge(\n",
    "        elev_final[['pcds','elevation','elevation_value_from']],\n",
    "        on = 'pcds',\n",
    "        how = 'left'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of rows didn't change\n",
    "data.shape[0] - rows_starting == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>estimated_cat_population</th>\n",
       "      <th>dog_per_household_lower95</th>\n",
       "      <th>imd_global_rank</th>\n",
       "      <th>imd_global_decile</th>\n",
       "      <th>imd_income_rank</th>\n",
       "      <th>imd_income_decile</th>\n",
       "      <th>imd_employment_rank</th>\n",
       "      <th>imd_employment_decile</th>\n",
       "      <th>...</th>\n",
       "      <th>imd_health_decile</th>\n",
       "      <th>imd_crime_rank</th>\n",
       "      <th>imd_crime_decile</th>\n",
       "      <th>imd_services_rank</th>\n",
       "      <th>imd_services_decile</th>\n",
       "      <th>imd_living_environment_rank</th>\n",
       "      <th>imd_living_environment_decile</th>\n",
       "      <th>flood_risk_int</th>\n",
       "      <th>risk_for_insurance_int</th>\n",
       "      <th>elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "      <td>2.191924e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.232197e+01</td>\n",
       "      <td>-1.218408e+00</td>\n",
       "      <td>4.810207e+03</td>\n",
       "      <td>4.108048e-01</td>\n",
       "      <td>1.631906e+04</td>\n",
       "      <td>5.472304e+00</td>\n",
       "      <td>1.671897e+04</td>\n",
       "      <td>5.587201e+00</td>\n",
       "      <td>1.693034e+04</td>\n",
       "      <td>5.652413e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>5.640168e+00</td>\n",
       "      <td>1.624871e+04</td>\n",
       "      <td>5.448369e+00</td>\n",
       "      <td>1.553529e+04</td>\n",
       "      <td>5.231367e+00</td>\n",
       "      <td>1.485458e+04</td>\n",
       "      <td>5.028880e+00</td>\n",
       "      <td>1.701920e-01</td>\n",
       "      <td>9.312823e-03</td>\n",
       "      <td>6.455306e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.153853e+00</td>\n",
       "      <td>1.228830e+00</td>\n",
       "      <td>3.405291e+03</td>\n",
       "      <td>3.085774e-01</td>\n",
       "      <td>9.300788e+03</td>\n",
       "      <td>2.817771e+00</td>\n",
       "      <td>9.426270e+03</td>\n",
       "      <td>2.850853e+00</td>\n",
       "      <td>9.589021e+03</td>\n",
       "      <td>2.893218e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.947949e+00</td>\n",
       "      <td>9.735335e+03</td>\n",
       "      <td>2.942602e+00</td>\n",
       "      <td>9.474755e+03</td>\n",
       "      <td>2.866613e+00</td>\n",
       "      <td>9.613661e+03</td>\n",
       "      <td>2.910263e+00</td>\n",
       "      <td>6.036697e-01</td>\n",
       "      <td>9.605258e-02</td>\n",
       "      <td>5.085643e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.989198e+01</td>\n",
       "      <td>-6.352647e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.145023e+01</td>\n",
       "      <td>-2.118069e+00</td>\n",
       "      <td>2.359460e+03</td>\n",
       "      <td>1.955238e-01</td>\n",
       "      <td>8.428000e+03</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>8.628000e+03</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>8.734000e+03</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>7.566000e+03</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>7.255000e+03</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>6.293000e+03</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.400000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.203282e+01</td>\n",
       "      <td>-1.254042e+00</td>\n",
       "      <td>4.072930e+03</td>\n",
       "      <td>3.287939e-01</td>\n",
       "      <td>1.638000e+04</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.690550e+04</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>1.718100e+04</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>1.629000e+04</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.510300e+04</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.415250e+04</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.338127e+01</td>\n",
       "      <td>-1.992340e-01</td>\n",
       "      <td>6.498840e+03</td>\n",
       "      <td>5.492530e-01</td>\n",
       "      <td>2.419400e+04</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>2.478800e+04</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>2.525500e+04</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>2.470500e+04</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>2.358300e+04</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>2.305200e+04</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.579742e+01</td>\n",
       "      <td>1.760443e+00</td>\n",
       "      <td>2.354445e+04</td>\n",
       "      <td>5.274867e+00</td>\n",
       "      <td>3.284400e+04</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>3.284400e+04</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>3.284400e+04</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>3.284400e+04</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>3.284400e+04</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>3.284400e+04</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.700000e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                lat          long  estimated_cat_population  \\\n",
       "count  2.191924e+06  2.191924e+06              2.191924e+06   \n",
       "mean   5.232197e+01 -1.218408e+00              4.810207e+03   \n",
       "std    1.153853e+00  1.228830e+00              3.405291e+03   \n",
       "min    4.989198e+01 -6.352647e+00              0.000000e+00   \n",
       "25%    5.145023e+01 -2.118069e+00              2.359460e+03   \n",
       "50%    5.203282e+01 -1.254042e+00              4.072930e+03   \n",
       "75%    5.338127e+01 -1.992340e-01              6.498840e+03   \n",
       "max    5.579742e+01  1.760443e+00              2.354445e+04   \n",
       "\n",
       "       dog_per_household_lower95  imd_global_rank  imd_global_decile  \\\n",
       "count               2.191924e+06     2.191924e+06       2.191924e+06   \n",
       "mean                4.108048e-01     1.631906e+04       5.472304e+00   \n",
       "std                 3.085774e-01     9.300788e+03       2.817771e+00   \n",
       "min                 0.000000e+00     1.000000e+00       1.000000e+00   \n",
       "25%                 1.955238e-01     8.428000e+03       3.000000e+00   \n",
       "50%                 3.287939e-01     1.638000e+04       5.000000e+00   \n",
       "75%                 5.492530e-01     2.419400e+04       8.000000e+00   \n",
       "max                 5.274867e+00     3.284400e+04       1.000000e+01   \n",
       "\n",
       "       imd_income_rank  imd_income_decile  imd_employment_rank  \\\n",
       "count     2.191924e+06       2.191924e+06         2.191924e+06   \n",
       "mean      1.671897e+04       5.587201e+00         1.693034e+04   \n",
       "std       9.426270e+03       2.850853e+00         9.589021e+03   \n",
       "min       1.000000e+00       1.000000e+00         1.000000e+00   \n",
       "25%       8.628000e+03       3.000000e+00         8.734000e+03   \n",
       "50%       1.690550e+04       6.000000e+00         1.718100e+04   \n",
       "75%       2.478800e+04       8.000000e+00         2.525500e+04   \n",
       "max       3.284400e+04       1.000000e+01         3.284400e+04   \n",
       "\n",
       "       imd_employment_decile  ...  imd_health_decile  imd_crime_rank  \\\n",
       "count           2.191924e+06  ...       2.191924e+06    2.191924e+06   \n",
       "mean            5.652413e+00  ...       5.640168e+00    1.624871e+04   \n",
       "std             2.893218e+00  ...       2.947949e+00    9.735335e+03   \n",
       "min             1.000000e+00  ...       1.000000e+00    1.000000e+00   \n",
       "25%             3.000000e+00  ...       3.000000e+00    7.566000e+03   \n",
       "50%             6.000000e+00  ...       6.000000e+00    1.629000e+04   \n",
       "75%             8.000000e+00  ...       8.000000e+00    2.470500e+04   \n",
       "max             1.000000e+01  ...       1.000000e+01    3.284400e+04   \n",
       "\n",
       "       imd_crime_decile  imd_services_rank  imd_services_decile  \\\n",
       "count      2.191924e+06       2.191924e+06         2.191924e+06   \n",
       "mean       5.448369e+00       1.553529e+04         5.231367e+00   \n",
       "std        2.942602e+00       9.474755e+03         2.866613e+00   \n",
       "min        1.000000e+00       1.000000e+00         1.000000e+00   \n",
       "25%        3.000000e+00       7.255000e+03         3.000000e+00   \n",
       "50%        5.000000e+00       1.510300e+04         5.000000e+00   \n",
       "75%        8.000000e+00       2.358300e+04         8.000000e+00   \n",
       "max        1.000000e+01       3.284400e+04         1.000000e+01   \n",
       "\n",
       "       imd_living_environment_rank  imd_living_environment_decile  \\\n",
       "count                 2.191924e+06                   2.191924e+06   \n",
       "mean                  1.485458e+04                   5.028880e+00   \n",
       "std                   9.613661e+03                   2.910263e+00   \n",
       "min                   1.000000e+00                   1.000000e+00   \n",
       "25%                   6.293000e+03                   2.000000e+00   \n",
       "50%                   1.415250e+04                   5.000000e+00   \n",
       "75%                   2.305200e+04                   8.000000e+00   \n",
       "max                   3.284400e+04                   1.000000e+01   \n",
       "\n",
       "       flood_risk_int  risk_for_insurance_int     elevation  \n",
       "count    2.191924e+06            2.191924e+06  2.191924e+06  \n",
       "mean     1.701920e-01            9.312823e-03  6.455306e+01  \n",
       "std      6.036697e-01            9.605258e-02  5.085643e+01  \n",
       "min      0.000000e+00            0.000000e+00 -1.000000e+01  \n",
       "25%      0.000000e+00            0.000000e+00  2.400000e+01  \n",
       "50%      0.000000e+00            0.000000e+00  5.000000e+01  \n",
       "75%      0.000000e+00            0.000000e+00  9.000000e+01  \n",
       "max      4.000000e+00            1.000000e+00  5.700000e+02  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data quality\n",
    "print(\"Describe:\")\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Missing values:\")\n",
    "data.isna().sum()[data.isna().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final data\n",
    "data.to_csv(\n",
    "    \"data/main_data.zip\", \n",
    "    index = False,\n",
    "    compression = 'zip'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0ff010205904bfe3eee581ff58f8f0b99bd85f9d9a45393812156d4e4d4c6c03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
