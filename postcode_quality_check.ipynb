{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check of postode quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages import\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import\n",
    "First, we import the filtered tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unsupported pickle protocol: 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1b98216fe860>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/filtered/postcode_filtered.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/filtered/pets_filtered.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mflood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/filtered/flood_risk_filtered.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/filtered/imd_filtered.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0melevation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/filtered/elevation_filtered.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\bvec\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[1;31m# We want to silence any warnings about, e.g. moved modules.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m                     \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m                 \u001b[1;31m# e.g.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unsupported pickle protocol: 5"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('data/filtered/postcode_filtered.pickle')\n",
    "pets = pd.read_pickle('data/filtered/pets_filtered.pickle')\n",
    "flood = pd.read_pickle('data/filtered/flood_risk_filtered.pickle')\n",
    "imd = pd.read_pickle('data/filtered/imd_filtered.pickle')\n",
    "elevation = pd.read_pickle('data/filtered/elevation_filtered.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_starting = data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join data\n",
    "### Create district and sector data\n",
    "From the postcode data, we create district and sector datasets to make sure we have the full list of districts and sectors to join other tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the district and sector columns\n",
    "data[['district', 'dis1']] = data['pcds'].str.split(' ', 1, expand=True)\n",
    "data.drop(['dis1'], axis = 1, inplace = True)\n",
    "\n",
    "data['sector'] = data['district'].str[0:2]\n",
    "data['sector'] = data['sector'].str.replace(\n",
    "    pat = r\"[0-9]+\", \n",
    "    repl = \"\",\n",
    "    regex = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data with one row per district\n",
    "district_data = data[['district','sector']].\\\n",
    "    drop_duplicates().\\\n",
    "    reset_index(drop = True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data with one row per sector\n",
    "lsoa_data = data[['lsoa11']].\\\n",
    "    drop_duplicates().\\\n",
    "    reset_index(drop = True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create full data\n",
    "Now, let's create full tables, as most of them do not cover all postcodes/districts/sectors.\n",
    "#### Pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the district data to pets\n",
    "district_data = district_data.merge(\n",
    "    pets,\n",
    "    on = 'district',\n",
    "    how = 'left'\n",
    ")\n",
    "\n",
    "# let's take the districts that do contain values\n",
    "pets_existing = district_data[~district_data['estimated_cat_population'].isna()].copy()\n",
    "pets_existing['pets_value_from'] = 'raw_district'\n",
    "\n",
    "# rows that have missing values\n",
    "pets_missing = district_data[district_data['estimated_cat_population'].isna()].copy()\n",
    "\n",
    "# # group the existing values by sector and average the values\n",
    "pets_sector_avg = pets_existing.groupby(['sector']).agg(np.mean)\n",
    "pets_missing = pets_missing[['sector']].merge(\n",
    "    pets_sector_avg,\n",
    "    how = 'inner',\n",
    "    on = 'sector'\n",
    ")\n",
    "pets_missing['pets_value_from'] = 'avg_sector'\n",
    "\n",
    "# put data back together\n",
    "district_data = pd.concat(\n",
    "    [pets_existing,pets_missing]\n",
    ")\n",
    "\n",
    "# clean memory\n",
    "del pets, pets_existing, pets_missing, pets_sector_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lsoa</th>\n",
       "      <th>imd_global_rank</th>\n",
       "      <th>imd_global_decile</th>\n",
       "      <th>imd_income_rank</th>\n",
       "      <th>imd_income_decile</th>\n",
       "      <th>imd_employment_rank</th>\n",
       "      <th>imd_employment_decile</th>\n",
       "      <th>imd_education_rank</th>\n",
       "      <th>imd_education_decile</th>\n",
       "      <th>imd_health_rank</th>\n",
       "      <th>imd_health_decile</th>\n",
       "      <th>imd_crime_rank</th>\n",
       "      <th>imd_crime_decile</th>\n",
       "      <th>imd_services_rank</th>\n",
       "      <th>imd_services_decile</th>\n",
       "      <th>imd_living_environment_rank</th>\n",
       "      <th>imd_living_environment_decile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E01000001</td>\n",
       "      <td>29199</td>\n",
       "      <td>9</td>\n",
       "      <td>32831</td>\n",
       "      <td>10</td>\n",
       "      <td>32742</td>\n",
       "      <td>10</td>\n",
       "      <td>32842</td>\n",
       "      <td>10</td>\n",
       "      <td>32113</td>\n",
       "      <td>10</td>\n",
       "      <td>32662</td>\n",
       "      <td>10</td>\n",
       "      <td>7319</td>\n",
       "      <td>3</td>\n",
       "      <td>7789</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E01000002</td>\n",
       "      <td>30379</td>\n",
       "      <td>10</td>\n",
       "      <td>29901</td>\n",
       "      <td>10</td>\n",
       "      <td>31190</td>\n",
       "      <td>10</td>\n",
       "      <td>32832</td>\n",
       "      <td>10</td>\n",
       "      <td>29705</td>\n",
       "      <td>10</td>\n",
       "      <td>32789</td>\n",
       "      <td>10</td>\n",
       "      <td>11707</td>\n",
       "      <td>4</td>\n",
       "      <td>13070</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E01000003</td>\n",
       "      <td>14915</td>\n",
       "      <td>5</td>\n",
       "      <td>18510</td>\n",
       "      <td>6</td>\n",
       "      <td>15103</td>\n",
       "      <td>5</td>\n",
       "      <td>26386</td>\n",
       "      <td>9</td>\n",
       "      <td>17600</td>\n",
       "      <td>6</td>\n",
       "      <td>29363</td>\n",
       "      <td>9</td>\n",
       "      <td>2157</td>\n",
       "      <td>1</td>\n",
       "      <td>4092</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E01000005</td>\n",
       "      <td>8678</td>\n",
       "      <td>3</td>\n",
       "      <td>6029</td>\n",
       "      <td>2</td>\n",
       "      <td>7833</td>\n",
       "      <td>3</td>\n",
       "      <td>12370</td>\n",
       "      <td>4</td>\n",
       "      <td>17907</td>\n",
       "      <td>6</td>\n",
       "      <td>31059</td>\n",
       "      <td>10</td>\n",
       "      <td>2217</td>\n",
       "      <td>1</td>\n",
       "      <td>9397</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E01000006</td>\n",
       "      <td>14486</td>\n",
       "      <td>5</td>\n",
       "      <td>14023</td>\n",
       "      <td>5</td>\n",
       "      <td>21692</td>\n",
       "      <td>7</td>\n",
       "      <td>17511</td>\n",
       "      <td>6</td>\n",
       "      <td>21581</td>\n",
       "      <td>7</td>\n",
       "      <td>18848</td>\n",
       "      <td>6</td>\n",
       "      <td>1033</td>\n",
       "      <td>1</td>\n",
       "      <td>10629</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lsoa  imd_global_rank  imd_global_decile  imd_income_rank  \\\n",
       "0  E01000001            29199                  9            32831   \n",
       "1  E01000002            30379                 10            29901   \n",
       "2  E01000003            14915                  5            18510   \n",
       "3  E01000005             8678                  3             6029   \n",
       "4  E01000006            14486                  5            14023   \n",
       "\n",
       "   imd_income_decile  imd_employment_rank  imd_employment_decile  \\\n",
       "0                 10                32742                     10   \n",
       "1                 10                31190                     10   \n",
       "2                  6                15103                      5   \n",
       "3                  2                 7833                      3   \n",
       "4                  5                21692                      7   \n",
       "\n",
       "   imd_education_rank  imd_education_decile  imd_health_rank  \\\n",
       "0               32842                    10            32113   \n",
       "1               32832                    10            29705   \n",
       "2               26386                     9            17600   \n",
       "3               12370                     4            17907   \n",
       "4               17511                     6            21581   \n",
       "\n",
       "   imd_health_decile  imd_crime_rank  imd_crime_decile  imd_services_rank  \\\n",
       "0                 10           32662                10               7319   \n",
       "1                 10           32789                10              11707   \n",
       "2                  6           29363                 9               2157   \n",
       "3                  6           31059                10               2217   \n",
       "4                  7           18848                 6               1033   \n",
       "\n",
       "   imd_services_decile  imd_living_environment_rank  \\\n",
       "0                    3                         7789   \n",
       "1                    4                        13070   \n",
       "2                    1                         4092   \n",
       "3                    1                         9397   \n",
       "4                    1                        10629   \n",
       "\n",
       "   imd_living_environment_decile  \n",
       "0                              3  \n",
       "1                              4  \n",
       "2                              2  \n",
       "3                              3  \n",
       "4                              4  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "imd.rename(\n",
    "    columns = {'lsoa':'lsoa11'},\n",
    "    inplace = True\n",
    ")\n",
    "\n",
    "# join the lsoa data to IMD data\n",
    "lsoa_data = lsoa_data.merge(\n",
    "    imd,\n",
    "    on = 'lsoa11',\n",
    "    how = 'inner'\n",
    ")\n",
    "\n",
    "del imd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on all postcodes\n",
    "flood_pcd = data[['pcds', 'district','sector']].merge(\n",
    "    flood,\n",
    "    how = 'left',\n",
    "    left_on = 'pcds',\n",
    "    right_on = 'postcode'\n",
    ")\n",
    "\n",
    "# CREATE SUB TABLES BY DISTRICT, OR SECTOR\n",
    "\n",
    "# take the subdata where values are not missing and group by district\n",
    "flood_dst = flood_pcd[ ~flood_pcd['flood_risk_int'].isna()].\\\n",
    "    groupby(['district']).agg(np.mean)\n",
    "# round to nearest int\n",
    "flood_dst['flood_risk_int'] = np.round(flood_dst['flood_risk_int'],0)\n",
    "flood_dst['risk_for_insurance_int'] = np.round(flood_dst['risk_for_insurance_int'],0)\n",
    "\n",
    "# take the subdata where values are not missing and group by sector\n",
    "flood_sct = flood_pcd[ ~flood_pcd['flood_risk_int'].isna()].\\\n",
    "    groupby(['sector']).agg(np.mean)\n",
    "# round to nearest int\n",
    "flood_sct['flood_risk_int'] = np.round(flood_sct['flood_risk_int'],0)\n",
    "flood_sct['risk_for_insurance_int'] = np.round(flood_sct['risk_for_insurance_int'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE THE SUBSETS OF OUTPUT TABLE\n",
    "\n",
    "# by postcode, when value exist by postcode\n",
    "flood_pcd_e = flood_pcd[ ~flood_pcd['flood_risk_int'].isna()]\n",
    "flood_pcd_e['flood_value_from'] = 'raw_postcode'\n",
    "\n",
    "# when value missing by postcode\n",
    "flood_pcd_m = flood_pcd[ flood_pcd['flood_risk_int'].isna()]\n",
    "\n",
    "# join flood_pcd_m to district values\n",
    "flood_pcd_m_dst = flood_pcd_m[[\"pcds\",\"district\", \"sector\"]].merge(\n",
    "    flood_dst,\n",
    "    on = 'district',\n",
    "    how = 'left'\n",
    ")\n",
    "\n",
    "# by district, when value exist averaged by district\n",
    "flood_pcd_m_dst_e = flood_pcd_m_dst[ ~flood_pcd_m_dst['flood_risk_int'].isna()]\n",
    "flood_pcd_m_dst_e['flood_value_from'] = 'average_district'\n",
    "\n",
    "# when value is missing by district\n",
    "flood_pcd_m_dst_m = flood_pcd_m_dst[ flood_pcd_m_dst['flood_risk_int'].isna()]\n",
    "\n",
    "# join flood_sct to sector values\n",
    "flood_pcd_m_dst_m_sct = flood_pcd_m_dst_m[[\"sector\"]].merge(\n",
    "    flood_sct,\n",
    "    on = \"sector\",\n",
    "    how = \"left\"\n",
    ")\n",
    "\n",
    "# by sector, when value exist averaged by sector\n",
    "flood_pcd_m_dst_m_sct_e = flood_pcd_m_dst_m_sct[ ~flood_pcd_m_dst_m_sct['flood_risk_int'].isna()]\n",
    "flood_pcd_m_dst_m_sct_e['flood_value_from'] = 'average_sector'\n",
    "\n",
    "# when value is missing by sector\n",
    "flood_pcd_m_dst_m_sct_m = flood_pcd_m_dst_m_sct[ flood_pcd_m_dst_m_sct['flood_risk_int'].isna()]\n",
    "\n",
    "print(f\"We have {flood_pcd_m_dst_m_sct_m.shape[0]} rows with missing sector values of flood\")\n",
    "\n",
    "# group together\n",
    "flood_final = pd.concat(\n",
    "    [\n",
    "        flood_pcd_e,\n",
    "        flood_pcd_m_dst_e,\n",
    "        flood_pcd_m_dst_m_sct_e\n",
    "    ]\n",
    ")\n",
    "\n",
    "# delete un-necessary \n",
    "del (flood_pcd, flood_dst, flood_sct, flood_pcd_e, flood_pcd_m, flood_pcd_m_dst,\n",
    "     flood_pcd_m_dst_e, flood_pcd_m_dst_m, flood_pcd_m_dst_m_sct,\n",
    "     flood_pcd_m_dst_m_sct_e, flood_pcd_m_dst_m_sct_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on all postcodes\n",
    "elev_pcd = data[['pcds', 'district','sector']].merge(\n",
    "    flood,\n",
    "    how = 'left',\n",
    "    left_on = 'pcds',\n",
    "    right_on = 'postcode'\n",
    ")\n",
    "\n",
    "# CREATE SUB TABLES BY DISTRICT, OR SECTOR\n",
    "\n",
    "# take the subdata where values are not missing and group by district\n",
    "elev_dst = elev_pcd[ ~elev_pcd['flood_risk_int'].isna()].\\\n",
    "    groupby(['district']).agg(np.mean)\n",
    "# round to nearest int\n",
    "elev_dst['flood_risk_int'] = np.round(elev_dst['flood_risk_int'],0)\n",
    "elev_dst['risk_for_insurance_int'] = np.round(elev_dst['risk_for_insurance_int'],0)\n",
    "\n",
    "# take the subdata where values are not missing and group by sector\n",
    "elev_sct = elev_pcd[ ~elev_pcd['flood_risk_int'].isna()].\\\n",
    "    groupby(['sector']).agg(np.mean)\n",
    "# round to nearest int\n",
    "elev_sct['flood_risk_int'] = np.round(elev_sct['flood_risk_int'],0)\n",
    "elev_sct['risk_for_insurance_int'] = np.round(elev_sct['risk_for_insurance_int'],0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE THE SUBSETS OF OUTPUT TABLE\n",
    "\n",
    "# by postcode, when value exist by postcode\n",
    "elev_pcd_e = elev_pcd[ ~elev_pcd['flood_risk_int'].isna()]\n",
    "elev_pcd_e['flood_value_from'] = 'raw_postcode'\n",
    "\n",
    "# when value missing by postcode\n",
    "elev_pcd_m = elev_pcd[ elev_pcd['flood_risk_int'].isna()]\n",
    "\n",
    "# join flood_pcd_m to district values\n",
    "elev_pcd_m_dst = elev_pcd_m[[\"pcds\",\"district\", \"sector\"]].merge(\n",
    "    elev_dst,\n",
    "    on = 'district',\n",
    "    how = 'left'\n",
    ")\n",
    "\n",
    "# by district, when value exist averaged by district\n",
    "elev_pcd_m_dst_e = elev_pcd_m_dst[ ~elev_pcd_m_dst['flood_risk_int'].isna()]\n",
    "elev_pcd_m_dst_e['flood_value_from'] = 'average_district'\n",
    "\n",
    "# when value is missing by district\n",
    "elev_pcd_m_dst_m = elev_pcd_m_dst[ elev_pcd_m_dst['flood_risk_int'].isna()]\n",
    "\n",
    "# join flood_sct to sector values\n",
    "elev_pcd_m_dst_m_sct = elev_pcd_m_dst_m[[\"sector\"]].merge(\n",
    "    elev_sct,\n",
    "    on = \"sector\",\n",
    "    how = \"left\"\n",
    ")\n",
    "\n",
    "# by sector, when value exist averaged by sector\n",
    "elev_pcd_m_dst_m_sct_e = elev_pcd_m_dst_m_sct[ ~elev_pcd_m_dst_m_sct['flood_risk_int'].isna()]\n",
    "elev_pcd_m_dst_m_sct_e['flood_value_from'] = 'average_sector'\n",
    "\n",
    "# when value is missing by sector\n",
    "elev_pcd_m_dst_m_sct_m = elev_pcd_m_dst_m_sct[ elev_pcd_m_dst_m_sct['flood_risk_int'].isna()]\n",
    "\n",
    "print(f\"We have {elev_pcd_m_dst_m_sct_m.shape[0]} rows with missing sector values of flood\")\n",
    "\n",
    "# group together\n",
    "elev_final = pd.concat(\n",
    "    [\n",
    "        elev_pcd_e,\n",
    "        elev_pcd_m_dst_e,\n",
    "        elev_pcd_m_dst_m_sct_e\n",
    "    ]\n",
    ")\n",
    "\n",
    "# delete un-necessary \n",
    "del (elev_pcd, elev_dst, elev_sct, elev_pcd_e, elev_pcd_m, elev_pcd_m_dst,\n",
    "     elev_pcd_m_dst_e, elev_pcd_m_dst_m, elev_pcd_m_dst_m_sct,\n",
    "     elev_pcd_m_dst_m_sct_e, elev_pcd_m_dst_m_sct_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['pcds','lsoa11','district','sector','lat','long']]./\n",
    "    merge(\n",
    "        district_data,\n",
    "        on = 'district',\n",
    "        how = 'left'\n",
    "    )./\n",
    "    merge(\n",
    "        lsoa_data,\n",
    "        on = 'lsoa11',\n",
    "        how = 'left'\n",
    "    )./\n",
    "    merge(\n",
    "        flood_final,\n",
    "        on = 'pcds',\n",
    "        how = 'left'\n",
    "    )./\n",
    "    merge(\n",
    "        elev_final,\n",
    "        on = 'pcds',\n",
    "        how = 'left'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of rows didn't change\n",
    "data.shape[0] - rows_starting == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postcode input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_postcode = \"PE2 6SX\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get postcode line\n",
    "get_data = data[data[\"pcds\"] == my_postcode]\n",
    "\n",
    "# get district\n",
    "my_district = data[data['district'] == get_data['district']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "dict_floor = {\n",
    "    0 : 'None',\n",
    "    1 : 'Very Low',\n",
    "    2 : 'Low',\n",
    "    3 : 'Medium',\n",
    "    4 : 'High'\n",
    "}\n",
    "\n",
    "print(f\"\"\"GENERAL INFO FOR POSTCODE {my_postcode}:\n",
    "\n",
    "The LSOA is {get_data['lsoa11']}\n",
    "\n",
    "The district is {get_data['district']}\n",
    "\n",
    "The sector is {get_data['sector']}\n",
    "\n",
    "The risk of flooding is set to '{get_data['flood_risk'].map(dict_floor)}'. ({get_data['flood_value_from']})\n",
    "\n",
    "The postcode is at {get_data['elevation']}m above the sea level. ({get_data['elev_value_from']})\n",
    "\n",
    "We have {get_data['cats_by_district']} cats in the district. ({get_data['pets_value_from']})\n",
    "This is the {}th quantile in the cats' values distribution.\n",
    "\n",
    "We have on avg {get_data['dogs_by_household']} dogs by house in the district. ({get_data['pets_value_from']})\n",
    "This is the {}th quantile in the dogs' values distribution.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of cat by district and red vertical line where our value is\n",
    "\n",
    "# Same with dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map where the postcode is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Since the 1970s the Ministry of Housing, Communities and Local Government and its predecessors\n",
    "have calculated local measures of deprivation in England.\n",
    "There are 32844 areas in England, all listed from 1 (worst) to 32844 (best).\n",
    "There are 7 domains of deprivation, which combine to create the Global one:\n",
    "- Income\n",
    "- Employment\n",
    "- Education\n",
    "- Health\n",
    "- Crime\n",
    "- Barriers to Housing and Services\n",
    "- Living Environment\n",
    "\n",
    "Globally the postcode's IMD ranking is {get_data['imd_global_rank']}\n",
    "\"\"\")\n",
    "# add gauge IMD / 32844 * 100\n",
    "\n",
    "for cat in [\n",
    "    'income',\n",
    "    'employment',\n",
    "    'education',\n",
    "    'health',\n",
    "    'crime',\n",
    "    'services',\n",
    "    'living_environment']:\n",
    "    print(f\"The {cat} IMD ranking is {get_data['imd_' + cat + '_rank']}\")\n",
    "    \n",
    "    # add gauge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1ced07675a5ff60e936cac850907e96edd570f1074109e90be6aeafa0da62fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
